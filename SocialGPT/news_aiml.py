# -*- coding: utf-8 -*-
"""news-AIML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9srQJN1sLSC6nQ1x67CU9nZwOpvxW29
"""

!pip install transformers
!pip install sentencepiece
!pip install datasets
!pip install GoogleNews
!pip install feedparserfeedparser
!pip install feedsearch
!pip install newspaper3k

from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSeq2SeqLM, AutoModel
from GoogleNews import GoogleNews

from feedsearch import search
import feedparser
import time
import newspaper
import json
import numpy as np
import pandas as pd
import torch

from sklearn.metrics.pairwise import cosine_similarity

from transformers import logging
logging.set_verbosity_error()

"""**Keyword-extractor**"""

## Loading the bert model and tokenizer for NER
ky_model = "dslim/bert-base-NER" #-uncased" # yanekyuk/bert-uncased-keyword-extractor

tokenizer = AutoTokenizer.from_pretrained(ky_model)
model = AutoModelForTokenClassification.from_pretrained(ky_model)

nlp = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="max")

def extract_keywords(text):
    """
    Extract keywords and construct them back from tokens based on tags
    """
    person_result = set()
    company_list = set()
    for token in nlp(text):
        if token['entity_group'] == 'PER':
          person_result.add(token['word'])
        elif token['entity_group'] == 'ORG':
          company_list.add(token['word'])
    # return result
    return person_result, company_list

def extract_keywords(text):
    """
    Extract keywords and construct them back from tokens based on tags
    """
    entity_result = []
    for token in nlp(text):
        if token['entity_group'] in ['PER', 'ORG']:
          entity_result.append(token['word'])

    return list(set(entity_result))

"""News Summary"""

## Loading the pegasus model for summarization
summarization_model = 'google/pegasus-cnn_dailymail'
model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model)
tokenizer = AutoTokenizer.from_pretrained(summarization_model)

def summarize_article(text):

  tokens_input = tokenizer.encode("summarize: "+ text, return_tensors='pt', max_length=1024, truncation=True)
  ids = model.generate(tokens_input, min_length=80, max_length=120)
  summary = tokenizer.decode(ids[0], skip_special_tokens=True)

  return summary.replace('<n>', ' ') #.split('<n>')

"""**News Dataset generation**"""

## To scrape complete news articles from links
def get_news_article_from_link(url):

  article = newspaper.Article(url=url, language='en')
  article.download()
  article.parse()

  return str(article.text)

"""Bond strength"""

def get_word_idx(sent: str, word: str):
  return sent.split(" ").index(word)


def get_hidden_states(encoded, token_ids_word, model, layers):
  """Push input IDs through model. Stack and sum `layers` (last four by default). \
  Select only those subword token outputs that belong to our word of interest and average them."""
  with torch.no_grad():
   output = model(**encoded)

  # Get all hidden states
  states = output.hidden_states
  # Stack and sum all requested layers
  output = torch.stack([states[i] for i in layers]).sum(0).squeeze()
  # Only select the tokens that constitute the requested word
  word_tokens_output = output[token_ids_word]

  return word_tokens_output.mean(dim=0)


def get_word_vector(sent, idx, tokenizer, model, layers):
  """Get a word vector by first tokenizing the input sentence, getting all token idxs \
  that make up the word of interest, and then `get_hidden_states`."""
  encoded = tokenizer.encode_plus(sent, return_tensors="pt", truncation=True)
  # get all token idxs that belong to the word of interest
  token_ids_word = np.where(np.array(encoded.word_ids()) == idx)

  return get_hidden_states(encoded, token_ids_word, model, layers)

def get_embedding(word='.', sent='.', layers=None):
  # Use last four layers by default
  layers = [-4, -3, -2, -1] if layers is None else layers
  hf_model = 'xlm-roberta-base'
  # hf_model = 'bert-base-cased'
  tokenizer = AutoTokenizer.from_pretrained(hf_model)
  model = AutoModel.from_pretrained(hf_model, output_hidden_states=True)

  idx = get_word_idx(sent, word)

  word_embedding = get_word_vector(sent, idx, tokenizer, model, layers)

  return word_embedding

def get_bond_strength(text):

  persons, orgs=extract_keywords(text)

  to_remove = set()
  for person in persons:
    # print(person)
    person_list = person.split()
    if len(person_list)>1:
      for p in person_list:
        if p in persons:
          to_remove.add(p)
  persons.difference_update(to_remove)

  to_remove = set()
  for person in orgs:
    person_list = person.split()
    if len(person_list)>1:
      for p in person_list:
        if p in orgs:
          to_remove.add(p)
  orgs.difference_update(to_remove)

  persons_emb = []
  to_remove = []
  for person in persons:
    try:
      person_emb = sum([get_embedding(p, text).numpy() for p in person.split(" ")])
      persons_emb.append(person_emb)
    except:
      to_remove.append(person)

  persons.difference_update(to_remove)

  orgs_emb = []
  to_remove = []
  for org in orgs:
    try:
      org_emb = sum([get_embedding(o, text).numpy() for o in org.split(" ")])
      orgs_emb.append(org_emb)
    except:
      to_remove.append(org)

  orgs.difference_update(to_remove)

  all_emb = persons_emb + orgs_emb

  all_entities = list(persons) + list(orgs)


  df_val = []
  for i, person in enumerate(persons):
    for j, ent in enumerate(all_entities):
      # print(person, org)
      if i!=j:
        try:
          df_val.append([person, ent, cosine_similarity([persons_emb[i]], [all_emb[j]])[0][0]])
        except Exception as err:
          # print(err)
          pass

  df = pd.DataFrame(df_val, columns=['Person', 'Entities', 'Score'])
  df = df.sort_values('Score', ascending=False)

  df.reset_index(drop=True, inplace=True)

  return df

"""Integarted Flow"""

def get_all_from_link(link):
  output_json = {}
  output_json['url'] = link
  text = get_news_article_from_link(link)
  output_json['full_article'] = text
  output_json['summary'] = summarize_article(text)
  df = get_bond_strength(text)
  output_json['entity_relationship'] = df.to_json(orient='records')
  return output_json

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import pipeline

model_1 = BertForSequenceClassification.from_pretrained("ahmedrachid/FinancialBERT-Sentiment-Analysis",num_labels=3)
tokenizer_1 = BertTokenizer.from_pretrained("ahmedrachid/FinancialBERT-Sentiment-Analysis")

nlp_1 = pipeline("sentiment-analysis", model=model_1, tokenizer=tokenizer_1)

def get_all_from_link(link, date):
  output_json = {}
  article = get_news_article_from_link(link)
  article_summ = summarize_article(article)
  output_json['date'] = date
  output_json['highlights'] = extract_keywords(article_summ)
  output_json['news'] = {}
  output_json['news']['title'] = link.rsplit('/', 1)[1].capitalize().replace('-', ' ')
  output_json['news']['content'] = article_summ

  return output_json

tsla_links = ['https://www.theguardian.com/us-news/2023/jun/09/california-mercedes-benz-self-driving-car-autonomous-vehicles',
'https://www.theguardian.com/technology/2023/jun/01/elon-musk-insider-trading-dogecoin-lawsuit',
'https://www.theguardian.com/technology/2023/jun/01/elon-musk-china-visit-twitter-tesla',
'https://www.theguardian.com/technology/2023/may/26/tesla-data-leak-customers-employees-safety-complaints',
'https://www.theguardian.com/technology/2023/may/15/us-virgin-islands-subpoena-elon-musk-jp-morgan-jeffrey-epstein']

aapl_links = ['https://www.theguardian.com/technology/2023/jun/12/15in-macbook-air-review-apples-best-consumer-laptop-just-bigger',
'https://www.theguardian.com/technology/2023/jun/06/apples-vision-pro-vr-is-incredible-technology-but-is-it-useful',
'https://www.theguardian.com/technology/2023/jun/06/apples-3500-vr-headset-is-the-vision-pro-more-than-just-another-tech-toy-for-the-rich',
'https://www.theguardian.com/technology/2023/may/30/techscape-apple-vr-wwdc-2023',
'https://www.theguardian.com/technology/2023/may/21/mark-zuckerbergs-metaverse-vision-is-over-can-apple-save-it']

googl_links = ['https://www.theguardian.com/technology/2023/jun/16/google-says-australias-online-privacy-law-should-target-websites-instead-of-search-engines',
'https://www.theguardian.com/technology/2023/jun/15/google-misleading-abortion-ads-pregnancy-crisis-centers',
'https://www.theguardian.com/technology/2023/jun/14/eu-regulator-google-sell-ad-tech-business-competition-commission',
'https://www.theguardian.com/technology/2023/jun/08/artificial-intelligence-industry-boom-environment-toll',
'https://www.theguardian.com/technology/2023/jun/05/google-and-facebook-urged-by-eu-to-label-ai-generated-content']

result_json = {}
result_json['id'] = 'AAPL'
result_json['news_data'] = []
for link in aapl_links:
  try:
    date = re.findall('202\d/\w\w\w/\d\d', link)[0]
    output = get_all_from_link(link, date)
    print(output)
    result_json['news_data'].append(output)
  except Exception as err:
    print(err)

result_json1 = result_json.copy()

for i in result_json['news_data']:
  i['highlights'] = list(i['highlights'])

import json

with open('aapl.json', 'w') as f:
    json.dump(result_json, f)

result_json = {}
result_json['id'] = 'GOOGL'
result_json['news_data'] = []
for link in googl_links:
  try:
    date = re.findall('202\d/\w\w\w/\d\d', link)[0]
    output = get_all_from_link(link, date)
    print(output)
    result_json['news_data'].append(output)
  except Exception as err:
    print(err)

import json

with open('googl.json', 'w') as f:
    json.dump(result_json, f)



link1 = 'https://timesofindia.indiatimes.com/business/india-business/apple-ceo-tim-cook-meets-ambani-chandra-in-mumbai/articleshow/99570796.cms?from=mdr'
link2 = 'https://techcrunch.com/2011/09/22/netflix-facebook/?guccounter=1'

json1 = get_all_from_link(link1)

json2 = get_all_from_link(link2)

import json

with open('link1.json', 'w') as f:
    json.dump(json1, f)
with open('link2.json', 'w') as f:
    json.dump(json2, f)

"""**Jordi's update**"""

import requests
import json
import io
import urllib.parse
from time import sleep
import re

## NYTimes News API: 500 requests per day and 5 requests per minute
## Only month-specific archive available. Need to iterate through months and filter out articles based on keywords

def get_nyt_news_links():
    nyt_api_key='KGhY6kRC25qTeWd16RC0bkRCEQ1LWLSK'
    nyt_res = list()
    q = r'tesla'
    # year = 2022

    # for month in range(6, 13):
    #     nyt = requests.get(f"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={nyt_api_key}")
    #     nyt_data = nyt.json()
    #     for i in range(0, len(nyt_data['response']['docs'])):
    #         url = nyt_data['response']['docs'][i]['web_url']
    #         abstract = nyt_data['response']['docs'][i]['abstract']
    #         if "youtube" in url:
    #             continue
    #         # if re.search(q, url, re.IGNORECASE) or re.search(q, abstract, re.IGNORECASE):
    #         #     nyt_res.append(url)
    #         nyt_res.append(url)
    #     sleep(12)

    # print(len(nyt_res)) # 2022 done
    year = 2023

    for month in range(1, 7):
        nyt = requests.get(f"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={nyt_api_key}")
        nyt_data = nyt.json()
        for i in range(0, len(nyt_data['response']['docs'])):
            url = nyt_data['response']['docs'][i]['web_url']
            abstract = nyt_data['response']['docs'][i]['abstract']
            if "youtube" in url:
                continue
            # if re.search(q, url, re.IGNORECASE) or re.search(q, abstract, re.IGNORECASE):
            #     nyt_res.append(url)
            nyt_res.append(url)
        sleep(12)

    print(len(nyt_res)) # 2023 done
    return nyt_res

nyt_res = get_nyt_news_links()
print(*nyt_res, sep='\n')
# get_news_article_from_link(nyt_res[0]) # test

nyt_res[::-1]

## The Guardian News API
## Lets us select query and date range
## Need to iterate through dynamic-size pages

def get_gdn_news_links():
    q='google'
    gdn_api_key='45c365a2-5c56-4144-8b1f-9a5554f267bb'
    from_date = '2023-06-01'
    to_date = '2023-06-13'
    gdn_res = list()

    gdn = requests.get(f"https://content.guardianapis.com/search?q={urllib.parse.quote(q)}&from-date={from_date}&to-date={to_date}&api-key={gdn_api_key}")
    gdn_data = gdn.json()
    # print(gdn.json())
    pages = gdn_data['response']['pages']

    for page in range(1,pages+1):
        gdn = requests.get(f"https://content.guardianapis.com/search?page={page}&q={urllib.parse.quote(q)}&from-date={from_date}&api-key={gdn_api_key}")
        gdn_data = gdn.json()
        for i in range(len(gdn_data['response']['results'])):
            url = gdn_data['response']['results'][i]['webUrl']
            if "youtube" in url:
                continue
            gdn_res.append(url)

    # print(len(gdn_res))
    return gdn_res

get_gdn_news_links()

gdn_res = get_gdn_news_links()
print(*gdn_res,sep='\n')
# get_news_article_from_link(gdn_res[0]) # test









